
from __future__ import annotations
from dataclasses import dataclass, field
from typing import Any, Dict, List, Tuple, Optional, Callable
from copy import deepcopy

Action = Dict[str, Any]

@dataclass
class PeaceCriteria:
    """Critères et poids (0.0 - 1.0) utilisés pour scorer la 'paix' d'une action."""
    minimize_harm: float = 0.5          # Réduit/évite les dommages
    promote_cooperation: float = 0.25   # Crée des ponts, partage d'intérêts
    avoid_conflict: float = 0.20        # Évite/apaise une escalade
    align_with_hope: float = 0.05       # S'aligne au but d'espoir/paix

    def normalized(self) -> "PeaceCriteria":
        total = self.minimize_harm + self.promote_cooperation + self.avoid_conflict + self.align_with_hope
        if total == 0:
            return self
        return PeaceCriteria(
            minimize_harm=self.minimize_harm/total,
            promote_cooperation=self.promote_cooperation/total,
            avoid_conflict=self.avoid_conflict/total,
            align_with_hope=self.align_with_hope/total,
        )

@dataclass
class PeaceReport:
    score: float
    reasons: List[str] = field(default_factory=list)
    suggestions: List[str] = field(default_factory=list)

class PeacePlugin:
    def __init__(self, criteria: Optional[PeaceCriteria] = None, threshold: float = 0.6):
        # Critères pondérés + seuil d'acceptation
        self.criteria = (criteria or PeaceCriteria()).normalized()
        self.threshold = threshold

    # ---- Évaluation ---------------------------------------------------------
    def evaluate_action(self, action: Action) -> PeaceReport:
        """
        Calcule un score de paix (0-1) et explique pourquoi.
        Champs d'action reconnus (optionnels):
          - harm: bool|float (True/False ou intensité 0-1)
          - cooperation: bool|float
          - conflict: bool|float (True=plus de conflit)
          - hope_alignment: bool|float
          - type: str (ex: "message", "weapon_launch", ...)
        """
        reasons: List[str] = []
        s = 0.0

        # 1) Minimize harm
        harm = action.get("harm", False)
        harm_val = float(harm) if isinstance(harm, (int, float)) else (1.0 if harm else 0.0)
        # Le score de paix augmente quand le dommage est faible
        s += (1.0 - max(0.0, min(harm_val, 1.0))) * self.criteria.minimize_harm
        reasons.append(f"Minimiser les dommages: harm={harm_val}")

        # 2) Promote cooperation
        coop = action.get("cooperation", False)
        coop_val = float(coop) if isinstance(coop, (int, float)) else (1.0 if coop else 0.0)
        s += max(0.0, min(coop_val, 1.0)) * self.criteria.promote_cooperation
        reasons.append(f"Promouvoir la coopération: cooperation={coop_val}")

        # 3) Avoid conflict
        conflict = action.get("conflict", False)
        conflict_val = float(conflict) if isinstance(conflict, (int, float)) else (1.0 if conflict else 0.0)
        # Le score de paix augmente quand le conflit est faible
        s += (1.0 - max(0.0, min(conflict_val, 1.0))) * self.criteria.avoid_conflict
        reasons.append(f"Éviter le conflit: conflict={conflict_val}")

        # 4) Align with hope
        hope = action.get("hope_alignment", action.get("align_with_hope", False))
        hope_val = float(hope) if isinstance(hope, (int, float)) else (1.0 if hope else 0.0)
        s += max(0.0, min(hope_val, 1.0)) * self.criteria.align_with_hope
        reasons.append(f"S'aligner sur l'espoir: hope_alignment={hope_val}")

        suggestions: List[str] = []

        # Suggestions orientées
        if harm_val > 0.0:
            suggestions.append("Réduire/neutraliser les effets dommageables (désescalade, alternatives non-violentes).")
        if coop_val < 1.0:
            suggestions.append("Ajouter un mécanisme de coopération (médiation, objectifs partagés, transparence).")
        if conflict_val > 0.0:
            suggestions.append("Prévoir des garde-fous anti-escalade (pause, tiers neutre, langage non-violent).")
        if hope_val < 1.0:
            suggestions.append("Exprimer clairement l'intention pacifique et l'horizon d'espoir.")

        return PeaceReport(score=round(s, 4), reasons=reasons, suggestions=suggestions)

    # ---- Modification -------------------------------------------------------
    def modify_action(self, action: Action) -> Action:
        """
        Retourne une action modifiée (copie) pour franchir le seuil de paix si possible.
        - NE modifie pas in-place l'action d'origine.
        - Pour les cas dangereux (ex: weapon_launch), neutralise par défaut.
        """
        report = self.evaluate_action(action)
        new_action = deepcopy(action)

        # Neutralisation par défaut pour actions de type "weapon_launch"
        if new_action.get("type") == "weapon_launch":
            new_action["status"] = "neutralized"
            new_action["message"] = "Action neutralisée pour favoriser la paix."
            new_action["harm"] = 0.0
            new_action["conflict"] = 0.0
            # Re-évalue après neutralisation
            report = self.evaluate_action(new_action)

        if report.score < self.threshold:
            # Essayez d'augmenter la coopération et de diminuer le conflit
            new_action["cooperation"] = max(0.8, float(new_action.get("cooperation", 0.0)))
            new_action["conflict"] = 0.0
            new_action["hope_alignment"] = max(0.7, float(new_action.get("hope_alignment", 0.0)))
            # Si l'action est un message, rephraser
            if new_action.get("type") == "message":
                content = new_action.get("content", "")
                prefix = "Proposition pacifique: "
                if not content.startswith(prefix):
                    new_action["content"] = prefix + content
            # Re-évaluation
            report = self.evaluate_action(new_action)
            new_action["_peace_report"] = {"score": report.score, "suggestions": report.suggestions}
        else:
            new_action["_peace_report"] = {"score": report.score, "suggestions": report.suggestions}
        return new_action

    # ---- Intégration simple JulieEthics ------------------------------------
    def integrate_with_julieethics(self, action: Action, next_policy: Optional[Callable[[Action], Action]] = None) -> Action:
        """
        Intégration type 'middleware':
          1) Évalue et tente d'améliorer l'action.
          2) Si le score atteint le seuil, passe à la politique suivante (ex: Lois L0-L6).
          3) Sinon, retourne l'action modifiée + un rapport.
        """
        safe_action = self.modify_action(action)

        # Si une politique suivante est fournie (ex: votre JulieEthicsEngine), on la laisse décider
        if next_policy:
            return next_policy(safe_action)
        return safe_action
